{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import dataUtils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = torch.load(\"Models/Final_models/2hid_32_neu_500epoch_p001lr_norm-11.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gridsearch data for normalisation\n",
    "(OUR MODEL TAKES TRANSLATIONS THEN ROTATIONS, GRID SEARCH DATA IS ROTATIONS THEN TRANSLATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES: 729 (729)\n",
      "INVALID FRAMES: 0, VALID FRAMES: 729\n"
     ]
    }
   ],
   "source": [
    "path_base = \"Tracking/Gridsearch/\"\n",
    "a = 100 #acceleration in steps\n",
    "hz = 30\n",
    "samples = 3 #steps in gridsearch\n",
    "stepsize = 20\n",
    "full_path = \"\"\n",
    "num_meta_rows = 6\n",
    "\n",
    "#load all data\n",
    "df = du.load_dataset(a,samples,stepsize,path_base,full_path)\n",
    "#print(df.head)\n",
    "\n",
    "# Change rotations to accurate coordinate system and make translations relative to frame 0\n",
    "df = du.fixCoordinates(df)\n",
    "\n",
    "#get valid frames (frames without wobeling/ocscilation)\n",
    "start_offset = 65 #(frame before action)\n",
    "wanted_frames = samples ** 6 #amount of frames to extract\n",
    "frame_offset = 2*hz #time between moves\n",
    "spacing = 10 #how many previous frames that have to be still\n",
    "t_eps = 0.5 #translational error\n",
    "r_eps =0.5 #rotational error\n",
    "\n",
    "df_valid, valid_idx = du.getValidFrames(df,wanted_frames,frame_offset,start_offset,spacing,t_eps,r_eps)\n",
    "X_norm = df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list sampling of isolated movements to feed as input to the neural network\n",
    "def getMoveRange(inc,max_val,move_type):\n",
    "  cur = -max_val\n",
    "  moves = []\n",
    "  while cur <= max_val:\n",
    "    tmp = [0,0,0,0,0,0]\n",
    "    tmp[move_type] = cur\n",
    "    moves.append(tmp)\n",
    "    cur += inc\n",
    "  moves.append([0,0,0,0,0,0])\n",
    "  return moves\n",
    "\n",
    "x_trans = getMoveRange(0.5,10,0)\n",
    "y_trans = getMoveRange(0.5,10,1)\n",
    "z_trans= getMoveRange(0.5,10,2)\n",
    "x_rot = getMoveRange(0.5,10,3)\n",
    "y_rot = getMoveRange(0.5,10,4)\n",
    "z_rot = getMoveRange(0.5,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularize the input data\n",
    "def raw_norm(data):\n",
    "  col = [\"X_trans\", \"Y_trans\", \"Z_trans\", \"X_rot\", \"Y_rot\", \"Z_rot\"]\n",
    "  data_norm = np.zeros(data.shape)\n",
    "  for i in range(6):\n",
    "    data_norm[:,i] = 2 * (data[:,i] - X_norm[col[i]].min()) / (X_norm[col[i]].max() - X_norm[col[i]].min()) - 1\n",
    "  return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8072095561730879 1.8050820970039614\n"
     ]
    }
   ],
   "source": [
    "# Provide input data (head poses, either ABCD data or custom data), given as translation then rotation\n",
    "data = np.array(x_trans + y_trans + z_trans + x_rot + y_rot + z_rot)\n",
    "#data = np.load(\"Data/ABCD/fake_abcd.npy\")[0]\n",
    "data_norm = raw_norm(data)\n",
    "\n",
    "print(np.min(data_norm),np.max(data_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(data_norm, dtype=torch.float32) #data_norm\n",
    "\n",
    "# Combine the input data and target values into a TensorDataset\n",
    "data_set = TensorDataset(data_tensor)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNumpy(tensors):\n",
    "  arr = [tensor.numpy() for tensor in tensors]\n",
    "  arr = np.array(arr)\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 18.811947    22.28018    -19.02961      7.3995705  -24.937191\n",
      "   -21.621122  ]]\n",
      "\n",
      " [[ 18.428333    22.018263   -18.03156      7.2339354  -24.328722\n",
      "   -21.175001  ]]\n",
      "\n",
      " [[ 17.951824    21.662191   -16.993744     6.973524   -23.70801\n",
      "   -20.774515  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-17.737474    16.466751   -23.214205    26.75773    -22.333054\n",
      "    11.8161955 ]]\n",
      "\n",
      " [[-17.447062    16.77441    -23.609985    27.640114   -23.391092\n",
      "    12.663159  ]]\n",
      "\n",
      " [[  1.7363617    1.4619877   -1.898098     0.5658432    0.25961065\n",
      "    -1.5611482 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Make setup that takes 1 person and gives cable lengths corresponding to that person\n",
    "\n",
    "preds = []\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs in data_loader:\n",
    "        outputs = model(inputs[0])\n",
    "        preds.append(outputs)\n",
    "        \n",
    "preds = toNumpy(preds)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which checks if any pose requires cables to be outside a specified range\n",
    "def inspect_tp_vals(tp,min_val,max_val):\n",
    "  print(f\"MAX {np.max(np.array(tp),axis=0)} MIN {np.min(np.array(tp),axis=0)}\")\n",
    "  for s,i in enumerate(tp):\n",
    "    for j in i:\n",
    "      if j > max_val or j < min_val:\n",
    "        print(i,s)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX [29 22 27 30 22 28] MIN [ -29  -27  -30  -27  -30 -144]\n",
      "[19, 22, -19, 7, -25, -22] 0\n",
      "[18, 22, -18, 7, -24, -21] 1\n",
      "[18, 22, -17, 7, -24, -21] 2\n",
      "[17, 21, -16, 7, -23, -20] 3\n",
      "[17, 21, -15, 6, -22, -20] 4\n",
      "[16, 20, -14, 6, -22, -20] 5\n",
      "[16, 19, -13, 5, -21, -19] 6\n",
      "[-18, -20, 10, -7, 21, 17] 36\n",
      "[-19, -22, 11, -7, 21, 17] 37\n",
      "[-19, -23, 12, -8, 21, 17] 38\n",
      "[-20, -24, 12, -8, 22, 17] 39\n",
      "[-21, -25, 12, -9, 22, 17] 40\n",
      "[-29, -21, -30, -26, -29, -31] 42\n",
      "[-29, -21, -28, -25, -28, -30] 43\n",
      "[-29, -20, -27, -25, -27, -29] 44\n",
      "[-29, -20, -26, -24, -26, -29] 45\n",
      "[-28, -20, -26, -24, -25, -27] 46\n",
      "[-26, -20, -25, -24, -24, -26] 47\n",
      "[-24, -19, -24, -23, -23, -24] 48\n",
      "[-22, -18, -23, -23, -21, -23] 49\n",
      "[-21, -18, -22, -22, -20, -22] 50\n",
      "[-19, -17, -20, -21, -18, -21] 51\n",
      "[20, 18, 18, 21, 16, 18] 72\n",
      "[21, 19, 19, 22, 17, 19] 73\n",
      "[22, 19, 20, 23, 17, 20] 74\n",
      "[23, 19, 21, 24, 18, 20] 75\n",
      "[24, 19, 22, 25, 19, 21] 76\n",
      "[25, 19, 23, 25, 19, 22] 77\n",
      "[26, 20, 24, 26, 20, 23] 78\n",
      "[27, 20, 25, 27, 20, 24] 79\n",
      "[28, 20, 26, 28, 20, 26] 80\n",
      "[29, 21, 26, 29, 20, 27] 81\n",
      "[29, 21, 27, 30, 21, 28] 82\n",
      "[-5, -9, 9, 9, -15, -88] 84\n",
      "[-5, -9, 9, 9, -14, -84] 85\n",
      "[-5, -8, 9, 9, -12, -80] 86\n",
      "[-5, -8, 9, 10, -11, -76] 87\n",
      "[-5, -8, 9, 10, -10, -71] 88\n",
      "[-5, -7, 8, 10, -9, -67] 89\n",
      "[-5, -6, 8, 11, -8, -63] 90\n",
      "[-5, -5, 8, 11, -7, -59] 91\n",
      "[-5, -5, 7, 12, -7, -54] 92\n",
      "[-4, -4, 7, 11, -5, -48] 93\n",
      "[-2, -3, 7, 9, -3, -42] 94\n",
      "[-1, -2, 6, 7, -2, -34] 95\n",
      "[0, -2, 5, 6, -3, -25] 96\n",
      "[-11, 2, -18, -14, -10, -144] 126\n",
      "[-11, 2, -18, -13, -9, -137] 127\n",
      "[-11, 2, -17, -13, -8, -130] 128\n",
      "[-12, 3, -17, -12, -6, -123] 129\n",
      "[-12, 3, -16, -10, -5, -116] 130\n",
      "[-12, 4, -16, -9, -4, -109] 131\n",
      "[-12, 4, -15, -8, -3, -102] 132\n",
      "[-12, 4, -15, -6, -2, -95] 133\n",
      "[-12, 5, -14, -5, 0, -88] 134\n",
      "[-12, 6, -13, -4, 1, -81] 135\n",
      "[-12, 7, -12, -3, 2, -74] 136\n",
      "[-12, 7, -12, -2, 3, -67] 137\n",
      "[-11, 8, -11, -1, 4, -60] 138\n",
      "[-9, 8, -11, -2, 5, -52] 139\n",
      "[-7, 9, -9, -3, 7, -42] 140\n",
      "[-3, 8, -7, -5, 8, -30] 141\n",
      "[13, -19, 9, 14, -21, 13] 159\n",
      "[13, -20, 10, 15, -22, 14] 160\n",
      "[14, -21, 11, 15, -24, 15] 161\n",
      "[15, -22, 12, 16, -25, 15] 162\n",
      "[16, -23, 12, 17, -26, 16] 163\n",
      "[17, -24, 13, 18, -27, 16] 164\n",
      "[18, -24, 14, 18, -29, 16] 165\n",
      "[19, -25, 15, 19, -30, 17] 166\n",
      "[-25, 20, 14, -12, -28, 8] 168\n",
      "[-24, 20, 14, -11, -28, 8] 169\n",
      "[-23, 20, 13, -11, -27, 8] 170\n",
      "[-22, 19, 13, -10, -26, 8] 171\n",
      "[-20, 19, 13, -9, -26, 8] 172\n",
      "[-17, 19, 12, -8, -24, 8] 173\n",
      "[-15, 18, 12, -8, -23, 8] 174\n",
      "[-12, 18, 12, -7, -21, 8] 175\n",
      "[8, -21, -16, 7, 19, -21] 203\n",
      "[8, -22, -16, 8, 20, -22] 204\n",
      "[8, -24, -17, 8, 20, -24] 205\n",
      "[8, -25, -17, 9, 20, -25] 206\n",
      "[8, -26, -18, 10, 20, -26] 207\n",
      "[9, -27, -18, 10, 20, -27] 208\n",
      "[23, -23, 26, -27, 18, -7] 210\n",
      "[22, -22, 25, -26, 17, -8] 211\n",
      "[21, -21, 25, -25, 17, -9] 212\n",
      "[20, -20, 24, -24, 16, -9] 213\n",
      "[19, -19, 23, -23, 16, -9] 214\n",
      "[18, -18, 22, -23, 15, -9] 215\n",
      "[17, -17, 21, -22, 14, -9] 216\n",
      "[16, -15, 19, -21, 13, -10] 217\n",
      "[-19, 15, -21, 21, -15, 7] 243\n",
      "[-19, 15, -21, 22, -17, 8] 244\n",
      "[-19, 15, -22, 23, -18, 8] 245\n",
      "[-19, 16, -22, 24, -19, 9] 246\n",
      "[-18, 16, -22, 25, -20, 10] 247\n",
      "[-18, 16, -23, 26, -21, 11] 248\n",
      "[-18, 16, -23, 27, -22, 12] 249\n",
      "[-17, 17, -24, 28, -23, 13] 250\n"
     ]
    }
   ],
   "source": [
    "#round each step and cast them to ints, also include a reset step at last\n",
    "data_steps = np.round(preds).astype(int)\n",
    "data_steps = data_steps.reshape((252,6))\n",
    "tps = list(map(list,data_steps))\n",
    "inspect_tp_vals(tps,-20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save array to feed to physical system using our protocol\n",
    "#np.save(\"Tracking/TPs/name.npy\", data_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
