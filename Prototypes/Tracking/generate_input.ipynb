{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import dataUtils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create np array sampling some range with some stepsize for each angle and translation\n",
    "\n",
    "# def get_input(max_move,step_size):\n",
    "#  arr = np.arange(-max_move,max_move+step_size,step_size)\n",
    "#  return arr\n",
    "#   # cur = 0\n",
    "#   # moves = []\n",
    "#   # for i in range(steps):\n",
    "#   #   temp = [0,0,0,0,0,0]\n",
    "#   #   temp[move_type] = cur\n",
    "\n",
    "#   #   moves.append()\n",
    "# a = get_input(7,1)\n",
    "# print(len(a))\n",
    "# print(a)\n",
    "def getMoveRange(inc,max_val,move_type):\n",
    "  cur = -max_val\n",
    "  moves = []\n",
    "  while cur <= max_val:\n",
    "    tmp = [0,0,0,0,0,0]\n",
    "    tmp[move_type] = cur\n",
    "    moves.append(tmp)\n",
    "    cur += inc\n",
    "  moves.append([0,0,0,0,0,0])\n",
    "  return moves\n",
    "\n",
    "x_trans = getMoveRange(0.5,10,0)\n",
    "y_trans = getMoveRange(0.5,10,1)\n",
    "z_trans= getMoveRange(0.5,10,2)\n",
    "x_rot = getMoveRange(0.5,10,3)\n",
    "y_rot = getMoveRange(0.5,10,4)\n",
    "z_rot = getMoveRange(0.5,10,5)\n",
    "\n",
    "data = x_trans + y_trans + z_trans + x_rot + y_rot + z_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        #self.bn1 = nn.BatchNorm1d(6)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        #self.bn2 = nn.BatchNorm1d(30)\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.bn3 = nn.BatchNorm1d(30)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.bn4 = nn.BatchNorm1d(30)\n",
    "        self.dropout3 = nn.Dropout(p=0.1)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "        self.fc5 = nn.Linear(hidden_size, hidden_size)\n",
    "        #self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        #x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        #x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        #x = self.dropout3(x)\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = torch.load(\"models/4hid_32_neu_100epoch_p001lr_norm-11_20rev.pth\")#_random20\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OUR MODEL TAKES ROT THEN TRANS, ABOVE DATA IS TRANS THEN ROT. WE NEED TO ACCOUNT FOR THIS\n",
    "\n",
    "# #Join\n",
    "# data = np.array(data)\n",
    "# print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES: 729 (729)\n",
      "INVALID FRAMES: 4, VALID FRAMES: 725\n"
     ]
    }
   ],
   "source": [
    "path_base = \"../Data/\"\n",
    "a = 100 #acceleration in steps\n",
    "hz = 30\n",
    "samples = 3 #steps in gridsearch\n",
    "stepsize = 20\n",
    "full_path = path_base + \"phantom_data_20step_markerAndS1Correct.csv\"\n",
    "num_meta_rows = 6\n",
    "\n",
    "#load all data\n",
    "df = du.load_dataset(a,samples,stepsize,path_base,full_path)\n",
    "#print(df.head)\n",
    "\n",
    "# Change rotations to accurate coordinate system and make translations relative to frame 0\n",
    "df = du.fixCoordinates(df)\n",
    "\n",
    "#get valid frames (frames without wobeling/ocscilation)\n",
    "start_offset = 80 #90 #(frame before action) #ORIGINAL = 65, 10step = 90, random20 = 83, 5step = 90\n",
    "wanted_frames = samples ** 6 #amount of frames to extract\n",
    "frame_offset = 2*hz #time between moves\n",
    "spacing = 10 #how many previous frames that have to be still\n",
    "t_eps = 0.5 #translational error\n",
    "r_eps =0.5 #rotational error\n",
    "\n",
    "df_valid, valid_idx = du.getValidFrames(df,wanted_frames,frame_offset,start_offset,spacing,t_eps,r_eps)\n",
    "X_norm = df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max = df_valid.max().values[2:]\n",
    "# min = df_valid.min().values[2:]\n",
    "# print(max)\n",
    "# print(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.586918114438728 1.4132700723652887\n"
     ]
    }
   ],
   "source": [
    "#Normalize data\n",
    "col = [\"X_trans\",\"Y_trans\",\"Z_trans\",\"X_rot\",\"Y_rot\",\"Z_rot\"]\n",
    "\n",
    "#data = np.array([[0,0,0,0,0,0],[0,0,0,5,0,0]])\n",
    "data = np.array([[0,0,0,-10,0,0],[0,0,0,-5,0,0],[0,0,0,-3,0,0],[0,0,0,0,0,0],[0,0,0,3,0,0],[0,0,0,5,0,0],[0,0,0,10,0,0]])\n",
    "#0,data = np.array([[0,0,0,0,-10,0],[0,0,0,0,-5,0],[0,0,0,0,-3,0],[0,0,0,0,0,0],[0,0,0,0,3,0],[0,0,0,0,5,0],[0,0,0,0,10,0]])\n",
    "data_norm = np.zeros(data.shape)\n",
    "\n",
    "for i in range(6):\n",
    "  data_norm[:,i] = 2 * (data[:,i] - X_norm[col[i]].min()) / (X_norm[col[i]].max() - X_norm[col[i]].min()) - 1\n",
    "\n",
    "\n",
    "print(np.min(data_norm),np.max(data_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(data_norm, dtype=torch.float32) #data_norm\n",
    "\n",
    "# Combine the input data and target values into a TensorDataset\n",
    "data_set = TensorDataset(data_tensor)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNumpy(tensors):\n",
    "  arr = [tensor.numpy() for tensor in tensors]\n",
    "\n",
    "  # Create a NumPy array containing these NumPy arrays\n",
    "  arr = np.array(arr)\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-5.5623318e+01  9.3809700e+00 -7.7673879e+00 -1.2137777e+01\n",
      "    8.8490047e+00 -8.5902176e+01]]\n",
      "\n",
      " [[-3.5869339e+01  7.7797608e+00 -3.1689639e+00 -7.8364973e+00\n",
      "    8.2932577e+00 -5.8460491e+01]]\n",
      "\n",
      " [[-2.0573063e+01  7.1100774e+00 -1.5568756e+00 -5.1178102e+00\n",
      "    8.4943056e+00 -4.0112709e+01]]\n",
      "\n",
      " [[ 2.0918477e+00 -3.5554200e-02  3.1788478e+00  2.3144820e-01\n",
      "    1.7708586e-01  1.2558415e+00]]\n",
      "\n",
      " [[ 8.2266655e+00 -1.0648525e+01  4.8272605e+00  4.2475939e+00\n",
      "   -1.0924109e+01  8.0278873e+00]]\n",
      "\n",
      " [[ 1.1983497e+01 -1.6989828e+01  5.3511472e+00  6.9090261e+00\n",
      "   -1.8326746e+01  1.2347897e+01]]\n",
      "\n",
      " [[ 1.8983227e+01 -2.9210548e+01  2.9990230e+00  1.3210029e+01\n",
      "   -3.2825665e+01  2.2647940e+01]]]\n"
     ]
    }
   ],
   "source": [
    "# Make setup that takes 1 person and gives cable lengths corresponding to that person\n",
    "\n",
    "preds = []\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for inputs in data_loader:\n",
    "        outputs = model(inputs[0])\n",
    "        preds.append(outputs)\n",
    "\n",
    "#print(preds[0])\n",
    "preds = toNumpy(preds)\n",
    "print(preds)\n",
    "#preds = preds.reshape(10,380,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 42 into shape (2,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_steps\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#10,380,6\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#tps = []\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#print(len(tps))\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m data_steps \u001b[38;5;241m=\u001b[39m \u001b[43mdata_steps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minspect_tp_vals\u001b[39m(tp,min_val,max_val):\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39marray(tp),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MIN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmin(np\u001b[38;5;241m.\u001b[39marray(tp),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 42 into shape (2,6)"
     ]
    }
   ],
   "source": [
    "#round each step and cast them to ints, also include a reset step at last\n",
    "data_steps = np.round(preds).astype(int)\n",
    "\n",
    "print(data_steps.shape)\n",
    "#10,380,6\n",
    "#tps = []\n",
    "\n",
    "# for i in range(data_steps.shape[0]):\n",
    "#   l = list(map(list,data_steps[i]))\n",
    "#   l.append([0,0,0,0,0,0])\n",
    "#   tps.append(l)\n",
    "\n",
    "#print(len(tps))\n",
    "data_steps = data_steps.reshape((2,6))\n",
    "def inspect_tp_vals(tp,min_val,max_val):\n",
    "  print(f\"MAX {np.max(np.array(tp),axis=0)} MIN {np.min(np.array(tp),axis=0)}\")\n",
    "  for s,i in enumerate(tp):\n",
    "    for j in i:\n",
    "      if j > max_val or j < min_val:\n",
    "        print(i,s)\n",
    "        break\n",
    "\n",
    "#for i in range(len(tps)):\n",
    "#print(f\"######## tp {i} ########\")\n",
    "#tps = tps.r\n",
    "tps = list(map(list,data_steps))\n",
    "inspect_tp_vals(tps,-20,20)\n",
    "\n",
    "#np.save(\"data/person_data/full_3hid_32_neu_500epoch_p001lr_norm-11_nod100.npy\",np.array(tps))\n",
    "#arr = np.array([1,2,3,4])\n",
    "#np.save(\"data/person_data/test.npy\",arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, -2, 1, 0, -2], [10, -14, 7, 11, -16, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(tps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.load(\"data/person_data/full_3hid_32_neu_500epoch_p001lr_norm-11.npy\")\n",
    "# print(test.shape)\n",
    "# print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
