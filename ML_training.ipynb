{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import dataUtils as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES: 729 (729)\n",
      "INVALID FRAMES: 0, VALID FRAMES: 729\n"
     ]
    }
   ],
   "source": [
    "path_base = \"Tracking/Gridsearch/\"\n",
    "a = 100 #acceleration in steps\n",
    "hz = 30\n",
    "samples = 3 #steps in gridsearch\n",
    "stepsize = 20 #steps used to perform gridsearch\n",
    "full_path = \"\" #custom path to gridsearch data\n",
    "num_meta_rows = 6\n",
    "\n",
    "#load all data\n",
    "df = du.load_dataset(a,samples,stepsize,path_base,full_path)\n",
    "\n",
    "# Change rotations to accurate coordinate system and make translations relative to frame 0\n",
    "df = du.fixCoordinates(df)\n",
    "\n",
    "#get valid frames (frames without wobeling/ocscilation)\n",
    "start_offset =  65 #(frame before action)\n",
    "wanted_frames = samples ** 6 #amount of frames to extract\n",
    "frame_offset = 2*hz #time between moves\n",
    "spacing = 10 #how many previous frames that are considered when testing for oscillation\n",
    "t_eps = 0.5 #translational error\n",
    "r_eps =0.5 #rotational error\n",
    "\n",
    "df_valid, valid_idx = du.getValidFrames(df,wanted_frames,frame_offset,start_offset,spacing,t_eps,r_eps)\n",
    "\n",
    "#load targets\n",
    "comb_list = du.getGridsearchCableLengths(stepsize)\n",
    "comb_list = du.getValidTargets(comb_list,valid_idx)\n",
    "Y = np.array(comb_list)\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame      43805.000000\n",
      "Time        1460.166667\n",
      "X_rot          7.778208\n",
      "Y_rot         10.491430\n",
      "Z_rot          7.510272\n",
      "X_trans        9.275269\n",
      "Y_trans        5.542541\n",
      "Z_trans       14.873856\n",
      "dtype: float64\n",
      "Frame      125.000000\n",
      "Time         4.166667\n",
      "X_rot       -6.316861\n",
      "Y_rot      -10.544286\n",
      "Z_rot       -7.188015\n",
      "X_trans     -8.202880\n",
      "Y_trans     -5.530762\n",
      "Z_trans    -11.188171\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.max())\n",
    "print(df_valid.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data representation and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularize the input data\n",
    "def raw(norm = True):\n",
    "  col = [\"X_trans\", \"Y_trans\", \"Z_trans\", \"X_rot\", \"Y_rot\", \"Z_rot\"]\n",
    "  X = df_valid[col].copy()\n",
    "  if norm:\n",
    "    for i in range(6):\n",
    "      X[col[i]] = 2 * (X[col[i]] - X[col[i]].min()) / (X[col[i]].max() - X[col[i]].min()) - 1\n",
    "\n",
    "\n",
    "  # Split data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X.values, Y, test_size=0.15, random_state=seed)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker normalize\n",
    "def marker(norm = True):\n",
    "  # min-max scale translations to be in [-1,1]\n",
    "  col = [ \"X_trans\", \"Y_trans\", \"Z_trans\"]\n",
    "  X_temp = df_valid.copy()\n",
    "  for c in col:\n",
    "    X_temp[c] = 2 * (X_temp[c] - X_temp[c].min()) / (X_temp[c].max() - X_temp[c].min()) - 1\n",
    "  # Selecting features from column index 2 to 7 (exclusive of 8)\n",
    "  X =X_temp.iloc[:, 2:8].values\n",
    "  rotations = X[:, :3]\n",
    "  translations = X[:, 3:]\n",
    "\n",
    "  # Define the positions of 4 points spaced around the original pivot point\n",
    "  # These positions are relative to the pivot point before rotation\n",
    "  point_offsets = np.array([[1, 0, 1],  # Point 1\n",
    "                            [0, 1, -1],  # Point 2\n",
    "                            [-1, -1, 0], # Point 3\n",
    "                            [-1, 0, -1]])# Point 4\n",
    "\n",
    "  # Convert Euler angles to rotation matrices\n",
    "  rot_matrices = R.from_euler('xyz', rotations, degrees=True).as_matrix()\n",
    "\n",
    "  # Apply rotation to the point offsets\n",
    "  rotated_points = np.matmul(rot_matrices, point_offsets.transpose())\n",
    "  rotated_points = np.transpose(rotated_points, axes=(0, 2, 1))\n",
    "\n",
    "\n",
    "  for i in range(rotated_points.shape[0]):\n",
    "    for point in rotated_points[i]:\n",
    "      point[0] += translations[i,0]\n",
    "      point[1] += translations[i,1]\n",
    "      point[2] += translations[i,2]\n",
    "\n",
    "  X = rotated_points.reshape(-1,12)\n",
    "  if norm:\n",
    "    # 12 features, xyz xyz xyz xyz\n",
    "    min_xs, max_xs = min([np.min(X[:,0]),np.min(X[:,3]),np.min(X[:,6]),np.min(X[:,9])]), max([np.max(X[:,0]),np.max(X[:,3]),np.max(X[:,6]),np.max(X[:,9])])\n",
    "    min_ys, max_ys = min([np.min(X[:,1]),np.min(X[:,4]),np.min(X[:,7]),np.min(X[:,10])]), max([np.max(X[:,1]),np.max(X[:,4]),np.max(X[:,7]),np.max(X[:,10])])\n",
    "    min_zs, max_zs = min([np.min(X[:,2]),np.min(X[:,5]),np.min(X[:,8]),np.min(X[:,11])]), max([np.max(X[:,2]),np.max(X[:,5]),np.max(X[:,8]),np.max(X[:,11])])\n",
    "\n",
    "    #normalize all vectors based on their max/min x,y or z value\n",
    "    for i in range(X.shape[0]):\n",
    "      for x in [0,3,6,9]:\n",
    "        X[i,x] = 2*((X[i,x]-min_xs)/(max_xs-min_xs)) - 1\n",
    "      for y in [1,4,7,10]:\n",
    "        X[i,y] = 2*((X[i,y]-min_ys)/(max_ys-min_ys)) -1\n",
    "      for z in [2,5,8,11]:\n",
    "        X[i,z] = 2*((X[i,z]-min_zs)/(max_zs-min_zs)) -1\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marker_split(norm = True):\n",
    "  # min-max scale translations to be in [-1,1]\n",
    "  col = [ \"X_trans\", \"Y_trans\", \"Z_trans\"]\n",
    "  X_norm = df_valid.copy()\n",
    "  if norm:\n",
    "    for c in col:\n",
    "      X_norm[c] = 2 * (X_norm[c] - X_norm[c].min()) / (X_norm[c].max() - X_norm[c].min()) - 1\n",
    "  # Selecting features from column index 2 to 7 (exclusive of 8)\n",
    "  X =X_norm.iloc[:, 2:8].values\n",
    "  rotations = X[:, :3]\n",
    "  translations = X[:, 3:]\n",
    "  # Define the positions of 4 points spaced around the original pivot point\n",
    "  # These positions are relative to the pivot point before rotation\n",
    "  point_offsets = np.array([[1, 0, 1],  # Point 1\n",
    "                            [0, 1, -1],  # Point 2\n",
    "                            [-1, -1, 0], # Point 3\n",
    "                            [-1, 0, -1]])# Point 4\n",
    "\n",
    "  # Convert Euler angles to rotation matrices\n",
    "  rot_matrices = R.from_euler('xyz', rotations, degrees=True).as_matrix()\n",
    "\n",
    "  # Apply rotation to the point offsets\n",
    "  rotated_points = np.matmul(rot_matrices, point_offsets.transpose())\n",
    "  rotated_points = np.transpose(rotated_points, axes=(0, 2, 1))\n",
    "  rotated_points = rotated_points.reshape(-1,12)\n",
    "  X =np.concatenate((rotated_points,translations),axis = 1) #makes translations/rotation be separate\n",
    "\n",
    "  if norm:\n",
    "  # vector normalize each \"rotation\" marker\n",
    "    for i in range(X.shape[0]):\n",
    "      for x in [0,3,6,9]:\n",
    "        X[i,x] = X[i,x] / np.sqrt(2)\n",
    "      for y in [1,4,7,10]:\n",
    "        X[i,y] = X[i,y] / np.sqrt(2)\n",
    "      for z in [2,5,8,11]:\n",
    "        X[i,z] = X[i,z] / np.sqrt(2)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = raw(norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE scores: [52.08849462365591, 55.9089247311828, 51.906129032258065, 54.14306451612904, 50.76130081300814]\n",
      "Mean Validation MSE: 52.96158274324679\n",
      "Std Validation MSE: 1.833042822557558\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RandomForestRegressor\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "\n",
    "# Specify the number of folds for cross-validation\n",
    "k_folds = 5\n",
    "\n",
    "# Initialize the KFold cross-validator\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Initialize list to store validation MSE scores\n",
    "validation_mse_scores = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Split the data into training and validation sets for this fold\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    random_forest.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = random_forest.predict(X_val_fold)\n",
    "    \n",
    "    # Compute MSE for this fold\n",
    "    mse = mean_squared_error(y_val_fold, y_pred)\n",
    "    validation_mse_scores.append(mse)\n",
    "\n",
    "# Print the validation MSE scores\n",
    "print(\"Validation MSE scores:\", validation_mse_scores)\n",
    "print(\"Mean Validation MSE:\", np.mean(validation_mse_scores))\n",
    "print(\"Std Validation MSE:\", np.std(validation_mse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Define input and output sizes\n",
    "input_size = 6 # 6 for raw, 12 for marker, 15 for marker_split\n",
    "output_size = 6\n",
    "hidden_size = 32\n",
    "batch_size = 4\n",
    "lr = 0.001\n",
    "num_epochs = 500\n",
    "\n",
    "# Create the model\n",
    "model = FeedForwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETUP DATALOADERS\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_data_loaders(train_index, val_index):\n",
    "    X_train_fold,y_train_fold = torch.tensor(X_train[train_index], dtype=torch.float32), torch.tensor(y_train[train_index], dtype=torch.float32)\n",
    "    X_val_fold,y_val_fold = torch.tensor(X_train[val_index], dtype=torch.float32), torch.tensor(y_train[val_index], dtype=torch.float32)\n",
    "    train_dataset = TensorDataset(X_train_fold,y_train_fold)\n",
    "    val_dataset = TensorDataset(X_val_fold,y_val_fold)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "i = 0\n",
    "criterion = nn.MSELoss()\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Initialize early stopping parameters\n",
    "    # best_val_loss = np.Inf\n",
    "    # patience = 15\n",
    "    # counter = 0\n",
    "\n",
    "    # Create the model\n",
    "    model = FeedForwardNN(input_size, hidden_size, output_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_losses_fold = []\n",
    "    val_losses_fold = []\n",
    "    train_loader,val_loader = create_data_loaders(train_index, val_index)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        # Compute average training loss for the epoch\n",
    "        average_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses_fold.append(average_train_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                running_val_loss += loss.item()\n",
    "        \n",
    "        # Compute average validation loss for the epoch\n",
    "        average_val_loss = running_val_loss / len(val_loader)\n",
    "        val_losses_fold.append(average_val_loss)\n",
    "\n",
    "         # Check for improvement in validation loss\n",
    "        # if running_val_loss < best_val_loss:\n",
    "        #     best_val_loss = running_val_loss\n",
    "        #     counter = 0\n",
    "        #     # Save the best model state if needed\n",
    "        #     # torch.save(model.state_dict(), 'best_model.pt')\n",
    "        # else:\n",
    "        #     counter += 1\n",
    "        #     # If no improvement for patience epochs, stop training\n",
    "        #     if counter >= patience:\n",
    "        #         print(f'Early stopping at epoch {epoch}.')\n",
    "        #         break\n",
    "        \n",
    "        # Print training and validation loss for the epoch\n",
    "        print(f\"Fold: {i}, Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}\", end = \"\\r\")\n",
    "    i += 1\n",
    "    train_losses.append(train_losses_fold)\n",
    "    val_losses.append(val_losses_fold)\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure()\n",
    "for i in range(k_folds):\n",
    "    plt.plot(train_losses[i], label='Train Loss')\n",
    "    plt.plot(val_losses[i], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE LOSS\n",
    "# np.save(\"data/20_new/3hid_64_neu_500epoch_p001lr_norm_trainloss\",np.array(train_losses))\n",
    "# np.save(\"data/20_new/3hid_64_neu_500epoch_p001lr_norm_valloss\",np.array(val_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on complete training data set (no validation), compared with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [166/500], Train Loss: 6.3742, Val Loss: 8.207779\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 35\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Compute average training loss for the epoch\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program_files\\anaconda3\\envs\\Thesis_sim\\lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Program_files\\anaconda3\\envs\\Thesis_sim\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\Program_files\\anaconda3\\envs\\Thesis_sim\\lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Program_files\\anaconda3\\envs\\Thesis_sim\\lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Program_files\\anaconda3\\envs\\Thesis_sim\\lib\\site-packages\\torch\\optim\\adam.py:377\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    373\u001b[0m         (param\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mis_xla \u001b[38;5;129;01mand\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_xla)\n\u001b[0;32m    374\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=True, params and state_steps must be CUDA or XLA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    380\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Lists to store training and validation losses\n",
    "\n",
    "# Create the model\n",
    "model = FeedForwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "train_data_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "targets_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "val_data_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "val_targets_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# Combine the input data and target values into a TensorDataset\n",
    "train_dataset = TensorDataset(train_data_tensor, targets_tensor)\n",
    "test_dataset = TensorDataset(val_data_tensor, val_targets_tensor)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    # Compute average training loss for the epoch\n",
    "    average_train_loss = running_train_loss / len(train_loader)\n",
    "    train_losses.append(average_train_loss)\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_val_loss += loss.item()\n",
    "    \n",
    "    # Compute average validation loss for the epoch\n",
    "    average_val_loss = running_val_loss / len(val_loader)\n",
    "    val_losses.append(average_val_loss)\n",
    "    \n",
    "    # Print training and validation loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_train_loss:.4f}, Val Loss: {average_val_loss:.4f}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SAVE MODEL\n",
    "#model_name = \"3hid_32_neu_500epoch_p001lr_norm-11_rev20\"\n",
    "#torch.save(model, \"models/\"+ model_name + \".pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
