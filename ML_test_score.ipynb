{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "import dataUtils as du"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL FRAMES: 729 (729)\n",
      "INVALID FRAMES: 0, VALID FRAMES: 729\n"
     ]
    }
   ],
   "source": [
    "path_base = \"Tracking/Gridsearch/\"\n",
    "a = 100 #acceleration in steps\n",
    "hz = 30\n",
    "samples = 3 #steps in gridsearch\n",
    "stepsize = 20 #steps used to perform gridsearch\n",
    "full_path = \"\" #custom path to gridsearch data\n",
    "num_meta_rows = 6\n",
    "\n",
    "#load all data\n",
    "df = du.load_dataset(a,samples,stepsize,path_base,full_path)\n",
    "\n",
    "# Change rotations to accurate coordinate system and make translations relative to frame 0\n",
    "df = du.fixCoordinates(df)\n",
    "\n",
    "#get valid frames (frames without wobeling/ocscilation)\n",
    "start_offset =  65 #(frame before action)\n",
    "wanted_frames = samples ** 6 #amount of frames to extract\n",
    "frame_offset = 2*hz #time between moves\n",
    "spacing = 10 #how many previous frames that are considered when testing for oscillation\n",
    "t_eps = 0.5 #translational error\n",
    "r_eps =0.5 #rotational error\n",
    "\n",
    "df_valid, valid_idx = du.getValidFrames(df,wanted_frames,frame_offset,start_offset,spacing,t_eps,r_eps)\n",
    "\n",
    "#load targets\n",
    "comb_list = du.getGridsearchCableLengths(stepsize)\n",
    "comb_list = du.getValidTargets(comb_list,valid_idx)\n",
    "Y = np.array(comb_list)\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NORMALIZE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularize the input data\n",
    "def raw(norm = True):\n",
    "  col = [\"X_trans\", \"Y_trans\", \"Z_trans\", \"X_rot\", \"Y_rot\", \"Z_rot\"]\n",
    "  X = df_valid[col].copy()\n",
    "  if norm:\n",
    "    for i in range(6):\n",
    "      X[col[i]] = 2 * (X[col[i]] - X[col[i]].min()) / (X[col[i]].max() - X[col[i]].min()) - 1\n",
    "\n",
    "\n",
    "  # Split data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X.values, Y, test_size=0.15, random_state=seed)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker normalize\n",
    "def marker(norm = True):\n",
    "  # min-max scale translations to be in [-1,1]\n",
    "  col = [ \"X_trans\", \"Y_trans\", \"Z_trans\"]\n",
    "  X_temp = df_valid.copy()\n",
    "  for c in col:\n",
    "    X_temp[c] = 2 * (X_temp[c] - X_temp[c].min()) / (X_temp[c].max() - X_temp[c].min()) - 1\n",
    "  # Selecting features from column index 2 to 7 (exclusive of 8)\n",
    "  X =X_temp.iloc[:, 2:8].values\n",
    "  rotations = X[:, :3]\n",
    "  translations = X[:, 3:]\n",
    "\n",
    "  # Define the positions of 4 points spaced around the original pivot point\n",
    "  # These positions are relative to the pivot point before rotation\n",
    "  point_offsets = np.array([[1, 0, 1],  # Point 1\n",
    "                            [0, 1, -1],  # Point 2\n",
    "                            [-1, -1, 0], # Point 3\n",
    "                            [-1, 0, -1]])# Point 4\n",
    "\n",
    "  # Convert Euler angles to rotation matrices\n",
    "  rot_matrices = R.from_euler('xyz', rotations, degrees=True).as_matrix()\n",
    "\n",
    "  # Apply rotation to the point offsets\n",
    "  rotated_points = np.matmul(rot_matrices, point_offsets.transpose())\n",
    "  rotated_points = np.transpose(rotated_points, axes=(0, 2, 1))\n",
    "\n",
    "\n",
    "  for i in range(rotated_points.shape[0]):\n",
    "    for point in rotated_points[i]:\n",
    "      point[0] += translations[i,0]\n",
    "      point[1] += translations[i,1]\n",
    "      point[2] += translations[i,2]\n",
    "\n",
    "  X = rotated_points.reshape(-1,12)\n",
    "  if norm:\n",
    "    # 12 features, xyz xyz xyz xyz\n",
    "    min_xs, max_xs = min([np.min(X[:,0]),np.min(X[:,3]),np.min(X[:,6]),np.min(X[:,9])]), max([np.max(X[:,0]),np.max(X[:,3]),np.max(X[:,6]),np.max(X[:,9])])\n",
    "    min_ys, max_ys = min([np.min(X[:,1]),np.min(X[:,4]),np.min(X[:,7]),np.min(X[:,10])]), max([np.max(X[:,1]),np.max(X[:,4]),np.max(X[:,7]),np.max(X[:,10])])\n",
    "    min_zs, max_zs = min([np.min(X[:,2]),np.min(X[:,5]),np.min(X[:,8]),np.min(X[:,11])]), max([np.max(X[:,2]),np.max(X[:,5]),np.max(X[:,8]),np.max(X[:,11])])\n",
    "\n",
    "    #normalize all vectors based on their max/min x,y or z value\n",
    "    for i in range(X.shape[0]):\n",
    "      for x in [0,3,6,9]:\n",
    "        X[i,x] = 2*((X[i,x]-min_xs)/(max_xs-min_xs)) - 1\n",
    "      for y in [1,4,7,10]:\n",
    "        X[i,y] = 2*((X[i,y]-min_ys)/(max_ys-min_ys)) -1\n",
    "      for z in [2,5,8,11]:\n",
    "        X[i,z] = 2*((X[i,z]-min_zs)/(max_zs-min_zs)) -1\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marker_split(norm = True):\n",
    "  # min-max scale translations to be in [-1,1]\n",
    "  col = [ \"X_trans\", \"Y_trans\", \"Z_trans\"]\n",
    "  X_norm = df_valid.copy()\n",
    "  if norm:\n",
    "    for c in col:\n",
    "      X_norm[c] = 2 * (X_norm[c] - X_norm[c].min()) / (X_norm[c].max() - X_norm[c].min()) - 1\n",
    "  # Selecting features from column index 2 to 7 (exclusive of 8)\n",
    "  X =X_norm.iloc[:, 2:8].values\n",
    "  rotations = X[:, :3]\n",
    "  translations = X[:, 3:]\n",
    "  # Define the positions of 4 points spaced around the original pivot point\n",
    "  # These positions are relative to the pivot point before rotation\n",
    "  point_offsets = np.array([[1, 0, 1],  # Point 1\n",
    "                            [0, 1, -1],  # Point 2\n",
    "                            [-1, -1, 0], # Point 3\n",
    "                            [-1, 0, -1]])# Point 4\n",
    "\n",
    "  # Convert Euler angles to rotation matrices\n",
    "  rot_matrices = R.from_euler('xyz', rotations, degrees=True).as_matrix()\n",
    "\n",
    "  # Apply rotation to the point offsets\n",
    "  rotated_points = np.matmul(rot_matrices, point_offsets.transpose())\n",
    "  rotated_points = np.transpose(rotated_points, axes=(0, 2, 1))\n",
    "  rotated_points = rotated_points.reshape(-1,12)\n",
    "  X =np.concatenate((rotated_points,translations),axis = 1) #makes translations/rotation be separate\n",
    "\n",
    "  if norm:\n",
    "  # vector normalize each \"rotation\" marker\n",
    "    for i in range(X.shape[0]):\n",
    "      for x in [0,3,6,9]:\n",
    "        X[i,x] = X[i,x] / np.sqrt(2)\n",
    "      for y in [1,4,7,10]:\n",
    "        X[i,y] = X[i,y] / np.sqrt(2)\n",
    "      for z in [2,5,8,11]:\n",
    "        X[i,z] = X[i,z] / np.sqrt(2)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=seed)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = raw(norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = torch.load(\"Models/Final_models/2hid_32_neu_500epoch_p001lr_norm-11.pth\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "L1 = nn.L1Loss()\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Setup data tensor\n",
    "train_data_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "targets_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "val_data_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "val_targets_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Combine the input data and target values into a TensorDataset\n",
    "train_dataset = TensorDataset(train_data_tensor, targets_tensor)\n",
    "test_dataset = TensorDataset(val_data_tensor, val_targets_tensor)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toNumpy(tensors):\n",
    "  arr = [tensor.numpy() for tensor in tensors]\n",
    "  arr = np.array(arr)\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL ON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Training MSE: 4.647031620168917, Mean Training STD: 8.214359908209905\n",
      "Mean Training L1: 1.1769864114786774, Mean Training STD: 0.6323048179990226\n",
      "Mean Test MSE: 7.238596547869119, Mean Test STD: 13.303230572219618\n",
      "Mean Test L1: 1.3606037670915776, Mean Test STD: 0.8504878335140799\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "# Lists to store training and validation losses\n",
    "val_pred = []\n",
    "val_target = []\n",
    "val_losses = []\n",
    "val_l1 = []\n",
    "train_pred = []\n",
    "train_target = []\n",
    "train_losses = []\n",
    "train_l1 = []\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "# best_val_loss = np.Inf\n",
    "# patience = 15\n",
    "# counter = 0\n",
    "\n",
    "# Validation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_val_loss = 0.0\n",
    "running_train_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        running_train_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        l1 = L1(outputs,targets)\n",
    "        train_l1.append(l1.item())\n",
    "        train_pred.append(outputs)\n",
    "        train_target.append(targets)\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        running_val_loss += loss.item()\n",
    "        val_losses.append(loss.item())\n",
    "        l1 = L1(outputs,targets)\n",
    "        val_l1.append(l1.item())\n",
    "        val_pred.append(outputs)\n",
    "        val_target.append(targets)\n",
    "# Compute average validation loss for the epoch\n",
    "average_val_loss = running_val_loss / len(val_loader)\n",
    "average_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "# Print training and validation loss for the epoch\n",
    "print(f\"Mean Training MSE: {average_train_loss}, Mean Training STD: {np.std(np.array(train_losses))}\")\n",
    "print(f\"Mean Training L1: {np.mean(np.array(train_l1))}, Mean Training STD: {np.std(np.array(train_l1))}\")\n",
    "print(f\"Mean Test MSE: {average_val_loss}, Mean Test STD: {np.std(np.array(val_losses))}\")\n",
    "print(f\"Mean Test L1: {np.mean(np.array(val_l1))}, Mean Test STD: {np.std(np.array(val_l1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = toNumpy(train_pred)\n",
    "train_target = toNumpy(train_target)\n",
    "test_pred = toNumpy(val_pred)\n",
    "test_target = toNumpy(val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: [ 1.9508692   0.6859009   1.9347792   0.5004356   0.58965886 22.220545  ]\n",
      "L1 Train: [0.9910788 0.6613938 0.8443406 0.5564867 0.5927506 3.4158654]\n",
      "MSE Test: [ 2.4328365   0.95128113  2.7785864   0.67148775  0.91180336 35.685577  ]\n",
      "L1 Test: [1.1293923  0.8010302  0.8824812  0.63860023 0.70215005 4.0099688 ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate MSE loss for each column\n",
    "mse_train = np.mean((train_pred - train_target)**2, axis=0)\n",
    "l1_train = np.mean((abs(train_pred - train_target)), axis=0)\n",
    "mse_test = np.mean((test_pred - test_target)**2, axis=0)\n",
    "l1_test = np.mean((abs(test_pred - test_target)), axis=0)\n",
    "print(f\"MSE Train: {mse_train[0]}\")\n",
    "print(f\"L1 Train: {l1_train[0]}\")\n",
    "print(f\"MSE Test: {mse_test[0]}\")\n",
    "print(f\"L1 Test: {l1_test[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
